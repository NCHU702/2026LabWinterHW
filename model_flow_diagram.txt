HydroNetRainOnly 模型資訊流圖 (最終版本)
=====================================

輸入 Input (來自 Dataset):
  x: (batch, seq_len, 1, H, W)
  實際例子: (8, 9, 1, 128, 128)
  
  說明:
  - batch=8: 批次大小
  - seq_len=9: 時間序列長度
    ├─ 時間步 0~5 (t-5 ~ t): 過去 6 小時的觀測降雨
    └─ 時間步 6~8 (t+1 ~ t+3): 未來 3 小時的預報降雨（含擾動）
  - 1: 單通道（降雨量）
  - H×W: 空間維度 (例如 128×128)


═══════════════════════════════════════════════════════════════════
  ENCODER - 逐時間步處理（t = 0 到 8，共 9 個時間步）
═══════════════════════════════════════════════════════════════════

初始化狀態:
  h1, c1: (batch, 16, H, W)     = (8, 16, 128, 128)
  h2, c2: (batch, 32, H//2, W//2) = (8, 32, 64, 64)

每個時間步 t 的處理流程:
  
  ┌────────────────────────────────────────────────────────┐
  │ 輸入: x[:, t]                                          │
  │ (batch, 1, H, W) = (8, 1, 128, 128)                   │
  └────────────────────┬───────────────────────────────────┘
                       │
         ╔═════════════╩═════════════╗
         ║    ENCODER LAYER 1        ║
         ╚═════════════╦═════════════╝
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ Conv2d(1→16, kernel=3, padding=1)                      │
  │ 提取空間特徵                                            │
  │ 輸出: (batch, 16, H, W) = (8, 16, 128, 128)           │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ LeakyReLU(0.2)                                         │
  │ conv_out1: (batch, 16, H, W) = (8, 16, 128, 128)      │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ ConvLSTMCell(input_dim=16, hidden_dim=16, kernel=3)    │
  │ 輸入: conv_out1 + (h1, c1)                             │
  │ 輸出: h1, c1 (更新後的狀態)                            │
  │       (batch, 16, H, W) = (8, 16, 128, 128)           │
  │                                                        │
  │ LSTM 門控機制:                                         │
  │   i_t = σ(W_i * [h_{t-1}, x_t])  # input gate        │
  │   f_t = σ(W_f * [h_{t-1}, x_t])  # forget gate       │
  │   o_t = σ(W_o * [h_{t-1}, x_t])  # output gate       │
  │   g_t = tanh(W_g * [h_{t-1}, x_t]) # cell gate       │
  │   c_t = f_t ⊙ c_{t-1} + i_t ⊙ g_t                    │
  │   h_t = o_t ⊙ tanh(c_t)                               │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ MaxPool2d(kernel=2, stride=2)                          │
  │ 空間下採樣                                              │
  │ pooled1: (batch, 16, H//2, W//2) = (8, 16, 64, 64)    │
  └────────────────────┬───────────────────────────────────┘
                       │
         ╔═════════════╩═════════════╗
         ║    ENCODER LAYER 2        ║
         ╚═════════════╦═════════════╝
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ Conv2d(16→32, kernel=3, padding=1)                     │
  │ 深層特徵提取                                            │
  │ 輸出: (batch, 32, H//2, W//2) = (8, 32, 64, 64)       │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ LeakyReLU(0.2)                                         │
  │ conv_out2: (batch, 32, H//2, W//2) = (8, 32, 64, 64)  │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ ConvLSTMCell(input_dim=32, hidden_dim=32, kernel=3)    │
  │ 輸入: conv_out2 + (h2, c2)                             │
  │ 輸出: h2, c2 (更新後的狀態)                            │
  │       (batch, 32, H//2, W//2) = (8, 32, 64, 64)       │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ MaxPool2d(kernel=2, stride=2)                          │
  │ 空間下採樣                                              │
  │ pooled2: (batch, 32, H//4, W//4) = (8, 32, 32, 32)    │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ 保存策略:                                              │
  │   if t >= 6:  # 時間步 6, 7, 8 (對應 t+1, t+2, t+3)   │
  │       h2_futures.append(pooled2)                       │
  │                                                        │
  │ 保存內容:                                              │
  │   h2_futures[0] = pooled2_{t+1}  # 時間步 6 的輸出    │
  │   h2_futures[1] = pooled2_{t+2}  # 時間步 7 的輸出    │
  │   h2_futures[2] = pooled2_{t+3}  # 時間步 8 的輸出    │
  └────────────────────────────────────────────────────────┘

                   循環 9 次，處理所有時間步
                   ↓
               完成 Encoder


═══════════════════════════════════════════════════════════════════
  DECODER - 對未來 3 個時間步分別解碼
═══════════════════════════════════════════════════════════════════

對於每個預測時間步 t ∈ {t+1, t+2, t+3}:

  ┌────────────────────────────────────────────────────────┐
  │ 輸入: h2_futures[t-1]                                  │
  │ (batch, 32, H//4, W//4) = (8, 32, 32, 32)             │
  │                                                        │
  │ 說明:                                                  │
  │   t+1 預測 ← h2_futures[0] (時間步 6 的 Block2 輸出)  │
  │   t+2 預測 ← h2_futures[1] (時間步 7 的 Block2 輸出)  │
  │   t+3 預測 ← h2_futures[2] (時間步 8 的 Block2 輸出)  │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ ConvTranspose2d(32→16, kernel=3, stride=2)             │
  │ 上採樣 2x (恢復空間解析度)                             │
  │ 輸出: (batch, 16, H//2, W//2) = (8, 16, 64, 64)       │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ LeakyReLU(0.2)                                         │
  │ d1: (batch, 16, H//2, W//2) = (8, 16, 64, 64)         │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ ConvTranspose2d(16→8, kernel=3, stride=2)              │
  │ 上採樣 2x (恢復到原始解析度)                           │
  │ 輸出: (batch, 8, H, W) = (8, 8, 128, 128)             │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ LeakyReLU(0.2)                                         │
  │ d2: (batch, 8, H, W) = (8, 8, 128, 128)               │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ Conv2d(8→1, kernel=3, padding=1)                       │
  │ 最終卷積，生成淹水增量預測                             │
  │ 輸出: (batch, 1, H, W) = (8, 1, 128, 128)             │
  └────────────────────┬───────────────────────────────────┘
                       │
                       ▼
  ┌────────────────────────────────────────────────────────┐
  │ 直接輸出（無激活函數）                                  │
  │ pred_t: (batch, 1, H, W) = (8, 1, 128, 128)           │
  │                                                        │
  │ 說明: 允許正值（水漲）和負值（水退）                   │
  │   正值: 淹水加深                                       │
  │   負值: 淹水退去                                       │
  │   零值: 水位不變                                       │
  │                                                        │
  │ 儲存到 predictions 列表                                │
  └────────────────────────────────────────────────────────┘

                  重複 3 次（t+1, t+2, t+3）
                           ↓

  ┌────────────────────────────────────────────────────────┐
  │ torch.stack(predictions, dim=1)                        │
  │ 堆疊所有預測結果                                        │
  │ out: (batch, 3, 1, H, W) = (8, 3, 1, 128, 128)        │
  └────────────────────────────────────────────────────────┘


輸出 Output:
  out: (batch, 3, 1, H, W) = (8, 3, 1, 128, 128)
  
  說明:
  - batch=8: 批次大小
  - 3: 未來 3 個時間步
    ├─ out[:, 0] → t+1 小時的淹水增量預測
    ├─ out[:, 1] → t+2 小時的淹水增量預測
    └─ out[:, 2] → t+3 小時的淹水增量預測
  - 1: 單通道（淹水增量深度）
  - H×W: 與輸入相同的空間維度


═══════════════════════════════════════════════════════════════════
  與 Dataset 的對應關係
═══════════════════════════════════════════════════════════════════

Dataset 輸出 (StochasticRainDataset.__getitem__):
  - input_pad:  [9, 1, H, W]  → DataLoader → [batch, 9, 1, H, W]
  - target_pad: [3, 1, H, W]  → DataLoader → [batch, 3, 1, H, W]
  - mask_pad:   [3, 1, H, W]  → DataLoader → [batch, 3, 1, H, W]

模型輸入/輸出:
  - 輸入: (batch, 9, 1, H, W)  ← 9 個時間步的降雨序列
  - 輸出: (batch, 3, 1, H, W)  ← 3 個時間步的淹水增量預測
  - 目標: (batch, 3, 1, H, W)  ← 實際淹水增量（ground truth）

完美匹配！✓


═══════════════════════════════════════════════════════════════════
  關鍵設計概念與架構邏輯
═══════════════════════════════════════════════════════════════════

1. 雙層 Encoder 架構：
   ┌─────────────────────────────────────────────────────┐
   │ Layer 1: 處理原始降雨 → 低層次時空特徵              │
   │ Layer 2: 處理 Layer 1 輸出 → 高層次時空特徵         │
   └─────────────────────────────────────────────────────┘
   
   • 每層都有獨立的 ConvLSTM，累積時序信息
   • Layer 2 的輸入是 Layer 1 當前時間步的輸出
   • 形成階層式特徵提取

2. 時序處理機制：
   ┌─────────────────────────────────────────────────────┐
   │ t=0: [t-5] → Conv → LSTM → Pool → Conv → LSTM → Pool│
   │ t=1: [t-4] → Conv → LSTM → Pool → Conv → LSTM → Pool│
   │ t=2: [t-3] → Conv → LSTM → Pool → Conv → LSTM → Pool│
   │ ...                                                 │
   │ t=6: [t+1] → Conv → LSTM → Pool → Conv → LSTM → Pool│ ← 保存
   │ t=7: [t+2] → Conv → LSTM → Pool → Conv → LSTM → Pool│ ← 保存
   │ t=8: [t+3] → Conv → LSTM → Pool → Conv → LSTM → Pool│ ← 保存
   └─────────────────────────────────────────────────────┘
   
   • 所有時間步共享 CNN 和 LSTM 權重
   • LSTM 狀態持續更新，捕捉時序依賴
   • 保存對應未來時間的 Block2 輸出用於預測

3. 多時間步預測策略：
   ┌─────────────────────────────────────────────────────┐
   │ t+1 預測 ← 使用時間步 6 的 Block2 輸出             │
   │ t+2 預測 ← 使用時間步 7 的 Block2 輸出             │
   │ t+3 預測 ← 使用時間步 8 的 Block2 輸出             │
   └─────────────────────────────────────────────────────┘
   
   • 每個預測對應其時間步的降雨輸入
   • Decoder 權重共享，處理不同時間步的特徵
   • 符合物理意義：t+k 的降雨影響 t+k 的淹水

4. 空間維度變化：
   ┌─────────────────────────────────────────────────────┐
   │ 輸入:    H × W      (128 × 128)                     │
   │ Pool1:   H/2 × W/2  (64 × 64)                       │
   │ Pool2:   H/4 × W/4  (32 × 32)                       │
   │ Upsample1: H/2 × W/2 (64 × 64)                      │
   │ Upsample2: H × W    (128 × 128)                     │
   └─────────────────────────────────────────────────────┘
   
   • 2 次下採樣 + 2 次上採樣
   • 保證輸入輸出空間維度一致

5. 通道數變化：
   ┌─────────────────────────────────────────────────────┐
   │ Encoder:  1 → 16 → 32                               │
   │ Decoder:  32 → 16 → 8 → 1                           │
   └─────────────────────────────────────────────────────┘
   
   • 逐層增加特徵複雜度
   • 最終收斂到單通道輸出

6. 激活函數選擇：
   ┌─────────────────────────────────────────────────────┐
   │ 中間層: LeakyReLU(0.2) - 避免死神經元               │
   │ 輸出層: ReLU() - 確保淹水深度非負                   │
   │ LSTM 內部: Sigmoid + Tanh - 標準 LSTM 門控          │
   └─────────────────────────────────────────────────────┘

7. 預測目標：
   ┌─────────────────────────────────────────────────────┐
   │ 預測的是淹水「增量」，而非累積值                     │
   │ target[t] = flood_depth[t] - flood_depth[t-1]       │
   │ 更容易學習，避免累積誤差                             │
   └─────────────────────────────────────────────────────┘


═══════════════════════════════════════════════════════════════════
  模型參數統計
═══════════════════════════════════════════════════════════════════

Encoder Layer 1:
  - Conv1:      1×16×3×3 + 16 = 160 參數
  - LSTM1:      (16+16)×(4×16)×3×3 = 18,432 參數
  
Encoder Layer 2:
  - Conv2:      16×32×3×3 + 32 = 4,640 參數
  - LSTM2:      (32+32)×(4×32)×3×3 = 73,728 參數

Decoder:
  - DecConv1:   32×16×3×3 + 16 = 4,624 參數
  - DecConv2:   16×8×3×3 + 8 = 1,160 參數
  - Final:      8×1×3×3 + 1 = 73 參數

總參數量: 約 102,817 參數

═══════════════════════════════════════════════════════════════════
  架構優勢
═══════════════════════════════════════════════════════════════════

✓ 時空特徵學習：ConvLSTM 同時處理空間和時序信息
✓ 階層式編碼：兩層 Encoder 提取不同層次的特徵
✓ 權重共享：參數效率高，減少過擬合風險
✓ 物理一致性：預測與對應時間步的降雨直接關聯
✓ 端到端訓練：無需複雜的後處理步驟
✓ 彈性擴展：易於增加更多層或修改通道數
